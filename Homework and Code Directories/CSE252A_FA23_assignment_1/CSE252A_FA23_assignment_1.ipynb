{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 252A Computer Vision I Fall 2023 - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor: Ben Ochoa\n",
    "\n",
    "Due: Wed, Oct 25, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Please answer the questions below using Python in the attached Jupyter notebook and follow the guidelines below:\n",
    " \n",
    "- This assignment must be completed **individually**. For more details, please follow the Academic Integrity Policy and Collaboration Policy on [Canvas](https://canvas.ucsd.edu).\n",
    "\n",
    "- All the solutions must be written in this Jupyter notebook.\n",
    "\n",
    "- You may use basic algebra packages (e.g., `NumPy`, `SciPy`, etc) but you are not allowed to use the packages that directly solve the problems. Feel free to ask the instructor and the teaching assistants if you are unsure about the packages to use.\n",
    "\n",
    "- It is highly recommended that you begin working on this assignment early.\n",
    "\n",
    "- **You must submit 3 files on Gradescope - `.pdf` , `.ipynb` and `.py` file where the .py file is the conversion of your .ipynb to .py file . You must mark each problem on Gradescope in the pdf.** \n",
    "    To convert the notebook to PDF, you can choose one way below:\n",
    "\n",
    "    1. You can print the web page and save as PDF (e.g., Chrome: Right click the web page $\\rightarrow$ Print... $\\rightarrow$ Choose \"Destination: Save as PDF\" and click \"Save\").\n",
    "\n",
    "    2. You can find the export option in the header: File $\\rightarrow$ Download as $\\rightarrow$ \"PDF via LaTeX\"\n",
    "\n",
    "    To convert the notebook (.ipynb) to .py file use the following command:\n",
    "\n",
    "<center> jupyter nbconvert --to script filename.ipynb --output output_filename.py </center>\n",
    "\n",
    "- Please make sure the content in each cell (e.g., code, output images, printed results, etc.) are clearly visible and are not cut-out or partially cropped in your final PDF file.\n",
    "\n",
    "- While submitting on gradescope, please make sure to assign the relevant pages in your PDF submission for each problem.\n",
    "\n",
    "**Late Policy:** Assignments submitted late will receive a 15% grade reduction for each 12 hours late (i.e., 30% per day). Assignments will not be accepted 72 hours after the due date. If you require an extension (for personal reasons only) to a due date, you must request one as far in advance as possible. Extensions requested close to or after the due date will only be granted for clear emergencies or clearly unforeseeable circumstances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Geometry (15 points)\n",
    "\n",
    "**Note:** The solution **must be** typed using Markdown or Latex, handwritten answers will not be accepted.\n",
    "\n",
    "Consider a line in the 2D plane, whose equation is given by $a\\tilde{x} + b\\tilde{y} + c = 0$, where $\\boldsymbol{l} = (a, b, c)^\\top$ and $\\boldsymbol{x} = (\\tilde{x}, \\tilde{y}, 1)^\\top$. Noticing that $\\boldsymbol{x}$ is a homogeneous representation of $\\tilde{\\boldsymbol{x}} = (\\tilde{x}, \\tilde{y})^\\top$, we can view $\\boldsymbol{l}$ as a homogeneous representation of the line $a\\tilde{x} + b\\tilde{y} + c = 0$.  We see that the line is also defined up to a scale since $(a, b, c)^\\top$ and  $k(a, b, c)^\\top$ with $k\\neq0$ represents the same line.\n",
    "\n",
    "\n",
    "\n",
    "1. [6 points] Prove $x^{T}l+l^{T}x=0$, if a point $x$ in homogeneous coordinates lies on the homogeneous line $l$.\n",
    "\n",
    "2. [2 points] What is the line, in homogenous coordinates, joining the inhomogeneous points $(3, 4)$ and $(7,-6)$.\n",
    "\n",
    "3. [2 points] Find a vector that is a homogeneous representation of the line that passes through the points $(-2, 6)$ and $(9, 1)$.  \n",
    "\n",
    "4. [5 points] Consider the intersection of two lines $l_1$ and $l_2$. Prove that the homogeneous point of intersection, $x$, of two homogeneous lines $l_1$ and $l_2$ is $x = l_1 \\times l_2$, where Ã— stands for the vector (or cross) product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Image Formation and Rigid Body Transformations (17 points)\n",
    "\n",
    "In  this  problem  we  will  practice  rigid  body  transformations  and  image  formations  through  the projective camera model. The goal will be to photograph the following four points \n",
    "$\\tilde{\\boldsymbol{X}}_1 = [\\text{2 4 -5}]^T$,  $\\tilde{\\boldsymbol{X}}_2 = [\\text{4 2 -5}]^T$, $\\tilde{\\boldsymbol{X}}_3 = [\\text{-2 -4 -5}]^T$, $\\tilde{\\boldsymbol{X}}_4 = [\\text{-4 -2 -5}]^T$ in the world coordinate frame. First, recall the following formula for rigid body transformation\n",
    "$$\n",
    "\\tilde{\\boldsymbol{X}}_{cam} = \\text{ } R\\tilde{\\boldsymbol{X}} +  \\boldsymbol{t}\n",
    "$$\n",
    "Where $\\tilde{\\boldsymbol{X}}_{cam}$ is the point coordinate in the camera coordinate system. $\\tilde{\\boldsymbol{X}}$ is a point in the world coordinate frame, and $\\text{R}$ and $\\boldsymbol{t}$ are the rotation and translation that transform points from the world coordinate frame to the camera coordinate frame. Together, $\\text{R}$ and $\\boldsymbol{t}$ are the $\\textit{extrinsic}$ camera parameters. Once transformed to the camera coordinate frame, the points can be photographed using the $3 \\times 3$ camera calibration matrix $\\text{K}$, which embodies the $\\textit{intrinsic}$ camera parameters, and the canonical projection matrix $[\\text{I} | \\boldsymbol{0}]$. Given $\\text{K}, \\text{R}$, and $\\boldsymbol{t}$, the image of a point $\\tilde{\\boldsymbol{X}}$ is $\\boldsymbol{x} = \\text{K}[\\text{I} | \\boldsymbol{0}]\\boldsymbol{X}_\\text{Cam} = \\text{K}[\\text{R} | \\boldsymbol{t}]\\boldsymbol{X}$, where the homogeneous points $\\boldsymbol{X}_\\text{Cam} = (\\tilde{\\boldsymbol{X}}_\\text{Cam}^\\top, 1)^\\top$ and $\\boldsymbol{X} = (\\tilde{\\boldsymbol{X}}^\\top, 1)^\\top$. We will consider four different settings of focal length, viewing angles and camera positions below. \n",
    "\n",
    "a). The extrinsic transformation matrix,\n",
    "\n",
    "b). Intrinsic camera matrix under the perspective camera assumption.\n",
    "\n",
    "c). Calculate the image of the four vertices and plot using the supplied **plot_points** function (see example output in figure below).\n",
    "\n",
    "<!--- ![fig3](fig3.png) --->\n",
    "<!--- The previous results in export to pdf errors on some systems but the following does not --->\n",
    "<img src=\"fig3.png\">\n",
    "\n",
    "1. [No rigid body transformation]. Focal  length  =  2. The  optical  axis  of  the  camera  is aligned with the z-axis.\n",
    "2. [Translation]. Focal  length  =  2. $\\boldsymbol{t} = [\\text{0 0 3}]^T$. The optical axis of the camera is aligned with the z-axis.\n",
    "3. [Translation and Rotation]. Focal length = 2. $\\text{}R$ encodes a 45 degrees around the z-axis and then 30 degrees around the y-axis. $\\boldsymbol{t} = [\\text{0 0 3}]^T$.\n",
    "4. [Translation and Rotation, long distance]. Focal length = 4. $\\text{}R$ encodes a 45 degrees around the z-axis and then 30 degrees around the y-axis. $\\boldsymbol{t} = [\\text{0 0 8}]^T$.\n",
    "\n",
    "We will not use a full intrinsic camera matrix (e.g.,  that maps centimeters to pixels, and specifies the coordinates of the principal point),  but  only  parameterize  this  with f,  the  focal\n",
    "length.  In other words:  the only parameter in the intrinsic camera matrix under the perspective assumption is f.\n",
    "\n",
    "For all the four cases, include a image like above.  Note that the axis are the same for each row, to facilitate comparison between the two camera models. Note: the angles and offsets used to generate these plots may be different from those in the problem statement, it's just to illustrate how to report your results.\n",
    "\n",
    "Also, Explain why you observe any distortions in the projection, if any, under this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def to_homog(points): \n",
    "    # convert points from inhomogeneous to homogeneous\n",
    "    # \n",
    "    # inputs: \n",
    "    # points is a dxn matrix where n is the number of d dimensional inhomogeneous points \n",
    "    # (e.g., d = 3 for 3D inhomogeneous points)\n",
    "    #\n",
    "    # outputs:\n",
    "    # homo_points is a (d+1)xn matrix of n d-dimensional homogeneous points\n",
    "    \n",
    "    # write your code here\n",
    "    \n",
    "    return homo_points\n",
    "\n",
    "\n",
    "def from_homog(points_homog):\n",
    "    # convert points from homogeneous to inhomogeneous\n",
    "    # \n",
    "    # inputs:\n",
    "    # points_homog is a (d+1)xn matrix of n d-dimensional homogeous points (e.g., d = 2 \n",
    "    # for 2D homogeneous points)\n",
    "    #\n",
    "    # outputs:\n",
    "    # inhomo_points is a dxn matrix of n d-dimensional inhomogeous points \n",
    "    \n",
    "    # write your code here\n",
    "    \n",
    "    return inhomog_points\n",
    "\n",
    "\n",
    "def project_points(P_int, P_ext, pts):\n",
    "    # project 3D inhomogeneous points to 2D inhomogeneous points\n",
    "    #\n",
    "    # inputs:\n",
    "    #    P_int - 3x3 intrinsic camera matrix\n",
    "    #    P_ext - 3x4 extrinsic camera matrix\n",
    "    #    pts - 3xn inhomogeneous points\n",
    "    #\n",
    "    # outputs:\n",
    "    #    pts_2d - 2xn inhomogeneous points\n",
    "    \n",
    "    # write your code here\n",
    "    \n",
    "    return pts_2d\n",
    "\n",
    "\n",
    "# Change the three matrices for the four cases as described in the problem\n",
    "# in the four camera functions given below. Make sure that we can see the formula\n",
    "# (if one exists) being used to fill in the matrices. Feel free to document with\n",
    "# comments any thing you feel the need to explain. \n",
    "\n",
    "\n",
    "def intrinsic_cam_mat(f):\n",
    "    \"\"\"\n",
    "    K = [f 0 0\n",
    "         0 f 0\n",
    "         0 0 1] \n",
    "    \"\"\"\n",
    "    # given the focal length, compute the intrinsic camera matrix\n",
    "    \n",
    "    # write your code here\n",
    "    return int_cam_mat\n",
    "\n",
    "def extrinsic_cam_mat(angles, t):\n",
    "    \"\"\"\n",
    "        ext_cam_mat = [R|t]\n",
    "    \"\"\"\n",
    "    # Compute the extrinsic camera matrix\n",
    "    #\n",
    "    # inputs:\n",
    "    #    angles - a tuple of angles (alpha, beta, gamma), representing the rotation\n",
    "    #    angles around x-axis, y-axis, and z-axis repectively in degrees.\n",
    "    #\n",
    "    # outputs:\n",
    "    #    ext_cam_mat - 3x4 extrinsic camera matrix\n",
    "    \n",
    "    \n",
    "    # write your code here\n",
    "    return ext_cam_mat\n",
    "\n",
    "def camera1():\n",
    "    \"\"\"\n",
    "    replace with your code\n",
    "    \"\"\"\n",
    "    # write your code here\n",
    "    return P_int_proj, P_ext\n",
    "\n",
    "def camera2():\n",
    "    \"\"\"\n",
    "    replace with your code\n",
    "    \"\"\"\n",
    "    # write your code here\n",
    "    return P_int_proj, P_ext\n",
    "\n",
    "def camera3():\n",
    "    \"\"\"\n",
    "    replace with your code\n",
    "    \"\"\"\n",
    "    # write your code here\n",
    "    return P_int_proj, P_ext\n",
    "\n",
    "def camera4():\n",
    "    \"\"\"\n",
    "    replace with your code\n",
    "    \"\"\"\n",
    "    # write your code here\n",
    "    return P_int_proj, P_ext\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# Do not modify\n",
    "#######################################################\n",
    "\n",
    "def plot_points(ax, points, title='', style='.-b', axis=[]):\n",
    "    inds = list(range(points.shape[1]))+[0]\n",
    "    ax.plot(points[0,inds], points[1,inds],style)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if axis:\n",
    "        ax.axis('scaled')\n",
    "        \n",
    "def main():\n",
    "    point1 = np.array([[2,4,-5]]).T\n",
    "    point2 = np.array([[4,2,-5]]).T\n",
    "    point3 = np.array([[-2,-4,-5]]).T\n",
    "    point4 = np.array([[-4,-2,-5]]).T\n",
    "    points = np.hstack((point1,point2,point3,point4))\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    for i, camera in enumerate([camera1, camera2, camera3, camera4]):\n",
    "        P_int_proj, P_ext = camera()\n",
    "        ax = fig.add_subplot(2,2,i+1)\n",
    "        plot_points(ax, project_points(P_int_proj, P_ext, points), title='Camera %d Projective'%(i+1), axis=[-1,1,-1,1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Surface Rendering [18 pts]\n",
    "\n",
    "In this portion of the assignment we will be exploring different methods of approximating local reflectance of\n",
    "objects in a scene. This last section of the homework will be an exercise in rendering surfaces. Here, you need use the surface normals and the masks from the provided pickle files, with various light sources, different materials, and using a number of reflectance models. For the sake of simplicity, multiple reflections of light rays, and occlusion of light rays due to object/scene can be ignored.\n",
    "\n",
    "### Data\n",
    "\n",
    "The surface normals and masks are to be loaded from the respective pickle files. For comparison, You should display the rendering results for both normals calculated from the original image and the diffuse components. There are 2 images that we will be playing with namely one of a sphere and the other of a pear.\n",
    "\n",
    "Masks serve the purpose as the binary_mask given to you last assignment0. Mask is a $H$x$W$ matrix with each element being either 1 or 0, indicating if a pixel at given location is sphere/pear or not. It is provided for you to remove the background of the rendered sphere/pear. \n",
    "\n",
    "Assume that the albedo map is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambertian Reflectance\n",
    "\n",
    "One of the simplest models available to render 3D objections with reflectance is the Lambertian model. This\n",
    "model finds the apparent brightness to an observer using the direction of the light source $\\hat{\\mathbf{s}}$ and the normal\n",
    "vector on the surface of the object $\\hat{\\mathbf{n}}$. The brightness intensity at a given point on an objectâ€™s surface, $e$, with\n",
    "a single light source is found using the following relationship:\n",
    "\n",
    "$$e = aC\\hspace{0.05cm} \\text{max}(0, \\hat{\\mathbf{n}}^\\top\\mathbf{s})$$\n",
    "\n",
    "$$\\mathbf{s} = s_0 \\hat{\\mathbf{s}}$$\n",
    "\n",
    "where, $a$ is the albedo of the surface facet imaged by pixel, $C$ is the light color, $s_0$ is the intensity of the light source and you can assume that the intensity is 1 here, and $\\hat{\\mathbf{s}}$ is the unit direction to the light source from the surface facet imaged by pixel.\n",
    "\n",
    "### Blinn-Phong Reflectance\n",
    "\n",
    "One major drawback of Lambertian reflectance is that it only considers the diffuse light in its calculation of\n",
    "brightness intensity. One other major component to reflectance rendering is the specular component. The\n",
    "specular reflectance is the component of light that is reflected in a single direction, as opposed to all directions,\n",
    "which is the case in diffuse reflectance. One of the most used models to compute surface brightness with specular\n",
    "components is the Blinn-Phong reflectance model. This model combines ambient lighting, diffused reflectance as well\n",
    "as specular reflectance to find the brightness on a surface. Blinn-Phong shading also considers the material in the scene\n",
    "which is characterized by four values: the ambient reflection constant ($k_a$), the diffuse reflection constant ($k_d$),\n",
    "the specular reflection constant ($k_s$) and $\\alpha$ the Blinn-Phong constant, which is the â€˜shininessâ€™ of an object. Furthermore,\n",
    "since the specular component produces â€˜raysâ€™, only some of which would be observed by a single observer, the\n",
    "observerâ€™s viewing direction $\\hat{\\mathbf{v}}$ must also be known. For some scene with known material parameters with $M$\n",
    "light sources the light intensity, $e$, on a surface with normal vector $\\hat{\\mathbf{n}}$ seen from viewing direction $\\hat{\\mathbf{v}}$ can be\n",
    "computed by:\n",
    "\n",
    "$$e = \\sum_{m\\in M}\\left\\{ s_{m,a}k_{a} + s_{m,d}k_{d} f_{d} + s_{m,s}k_{s}f_{s} \\right\\}\\text{,}$$\n",
    "\n",
    "$$f_{d} = \\text{max}(0, \\hat{\\mathbf{n}}^\\top \\hat{\\mathbf{s}})\\text{,}\\hspace{0.5cm} f_{s} = \\text{max}(0, \\hat{\\mathbf{n}}^\\top\\mathbf{\\hat{h}})^{\\alpha}$$\n",
    "\n",
    "$$\\mathbf{\\hat{h}} = \\frac{\\mathbf{h}}{||\\mathbf{h}||}\\text{,}\\hspace{0.5cm} \\mathbf{h}=\\hat{\\mathbf{s}}+\\hat{\\mathbf{v}}$$\n",
    "\n",
    "where $s_{m,a}$, is the ambient light intensity, $s_{m,d}$ and $s_{m,s}$ are the intensity of the the diffuse and\n",
    "specular light respectively for the $m$th light source.\n",
    "\n",
    "### Rendering\n",
    "\n",
    "Please complete the following:\n",
    "\n",
    "1. Write the function `lambertian()` that calculates the Lambertian light intensity given the light direction $\\hat{\\mathbf{s}}$ with intensity $s_0$, and normal vector $\\hat{\\mathbf{n}}$. Then use this function in a program that calculates and displays the specular sphere and the pear using each of the two lighting sources found in Table 1. *Note: You do not need to worry about material coefficients in this model.*\n",
    "\n",
    "1. Write the function `blinn_phong()` that calculates the Blinn-Phong light intensity given the material constants $(k_a, k_d, k_s, \\alpha)$, $\\hat{\\mathbf{v}} = (0, 0, 1)^\\top$, $\\hat{\\mathbf{n}}$ and some number of $M$ light sources. Then use this function in a program that calculates and displays the specular sphere and the pear using each of the sets of coefficients found in Table 2 with each light source individually, and both light sources combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Table 1: Light Sources\n",
    "\n",
    "| $m$ | Location | Color (RGB)  |\n",
    "| - | ----------- | ----- |\n",
    "| 1 | $(-\\tfrac{1}{2},\\tfrac{1}{2},\\tfrac{1}{2})^{\\top}$ | $(1,1,1)$ |\n",
    "| 2 | $(1,0,0)^{\\top}$     | $(1,.45,1)$ |\n",
    "\n",
    "Table 2: Material Coefficients\n",
    "\n",
    "| Mat. | $k_a$ | $k_d$ | $k_s$ | $\\alpha$ |\n",
    "| - | -------- | ----- | ----- | -------- |\n",
    "| 1 | $0$ | $0.1$ | $0.5$ | $5$ |\n",
    "| 2 | $0$ | $0.5$ | $0.1$ | $5$ |\n",
    "| 3 | $0$ | $0.5$ | $0.5$ | $10$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Plot the normals [4 pts] (Sphere - 2pts, Pear - 2pts)\n",
    "In this first part, you are required to work with 2 images, one of a sphere and the other one of a pear. The pickle file normals.pickle is a list consisting of 4 numpy matrices which are    \n",
    "1) Normal Vectors for the sphere with specularities removed (Diffuse component)  \n",
    "2) Normal Vector for the sphere    \n",
    "3) Normal Vectors for the pear with specularities removed (Diffuse component)  \n",
    "4) Normal vectors for the pear  \n",
    "\n",
    "Please plot the normals using the function plot_normals which is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normals(diffuse_normals, original_normals):\n",
    "    # Stride in the plot, you may want to adjust it to different images\n",
    "    stride = 5\n",
    "    \n",
    "    normalss = diffuse_normals\n",
    "    normalss1 = original_normals\n",
    "    \n",
    "    print(\"Normals:\")\n",
    "    print(\"Diffuse\")\n",
    "    # showing normals as three separate channels\n",
    "    figure = plt.figure()\n",
    "    ax1 = figure.add_subplot(131)\n",
    "    ax1.imshow(normalss[..., 0])\n",
    "    ax2 = figure.add_subplot(132)\n",
    "    ax2.imshow(normalss[..., 1])\n",
    "    ax3 = figure.add_subplot(133)\n",
    "    ax3.imshow(normalss[..., 2])\n",
    "    plt.show()\n",
    "    print(\"Original\")\n",
    "    figure = plt.figure()\n",
    "    ax1 = figure.add_subplot(131)\n",
    "    ax1.imshow(normalss1[..., 0])\n",
    "    ax2 = figure.add_subplot(132)\n",
    "    ax2.imshow(normalss1[..., 1])\n",
    "    ax3 = figure.add_subplot(133)\n",
    "    ax3.imshow(normalss1[..., 2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the normals for the sphere and pear for both the normal and diffuse components.\n",
    "#1 : Load the different normals\n",
    "import pickle\n",
    "\n",
    "with open('normals.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "#2 : Plot the normals using plot_normals\n",
    "# What do you observe? What are the differences between the diffuse component and the original images shown?\n",
    "# (Just something to think about, no need to provide an answer, but feel free to add markdown cells to explain your thoughts)\n",
    "\n",
    "#PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Lambertian model [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in your implementation for the rendered image using the lambertian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    assert img.shape[2] == 3\n",
    "    maxi = img.max()\n",
    "    mini = img.min()\n",
    "    return (img - mini)/(maxi-mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambertian(normals, lights, color, intensity, mask):\n",
    "    image = np.zeros((normals.shape[0], normals.shape[1], 3))\n",
    "    \n",
    "    '''Your implementation'''\n",
    "    \n",
    "    \n",
    "    return (image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the rendered results for both the sphere and the pear for both the original and the diffuse components. Remember to first load the masks from the masks.pkl file. The masks.pkl file is a list consisting of 2 numpy arrays-  \n",
    "1)Mask for the sphere  \n",
    "2)Mask for the pear  \n",
    "\n",
    "Plot the normalized image using the function normalize which is provided.\n",
    "\n",
    "With 2 light directions and 2 light colors, we expect 4 images for Pear and 4 images for Sphere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the masks for the sphere and pear\n",
    "with open('masks.pkl', 'rb') as h:\n",
    "    data1 = pickle.load(h)\n",
    "\n",
    "# Output the rendering results for Pear\n",
    "# Read light direction and color from the table\n",
    "dirn1 = np.zeros([3,1])\n",
    "color1 = np.zeros([1,3])\n",
    "dirn2 = np.zeros([3,1])\n",
    "color2 = np.zeros([1,3])\n",
    "\n",
    "#Display the rendering results for pear for both diffuse and for both the light sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the rendering results for Sphere\n",
    "dirn1 = np.zeros([3,1])\n",
    "color1 = np.zeros([1,3])\n",
    "dirn2 = np.zeros([3,1])\n",
    "color2 = np.zeros([1,3])\n",
    "#Display the rendering results for sphere for both diffuse and for both the light sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Blinn-Phong model [8 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your implementation for the Blinn-Phong model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blinn_phong(normals, lights, color, material, view, mask):\n",
    "    '''Your implementation'''\n",
    "    return (image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function completed, plot the rendering results for the sphere and pear (both diffuse and original components) for all the materials and light sources and also with the combination of both the light sources.\n",
    "\n",
    "With 2 light sources and 3 materials, we expect 9 images each for diffuse and original components. In total, 3\\*3\\*2 = 18 images for sphere and 18 images for pear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the rendering results for sphere\n",
    "view =  np.array([[0],[0],[1]])\n",
    "material = np.array([[0.1,0.5,5],[0.5,0.1,5],[0.5,0.5,10]])\n",
    "lightcol1 =  np.array([[1.0/2,1],[-1.0/2,1],[1.0/2,1]])\n",
    "lightcol2 = np.array([[0,1],[1,1],[0,0.5]])\n",
    "#Display rendered results for sphere for all materials and light sources and combination of light sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the rendering results for the pear.\n",
    "view =  np.array([[0],[0],[1]])\n",
    "material = np.array([[0.1,0.5,5],[0.5,0.1,5],[0.5,0.5,10]])\n",
    "lightcol1 =  np.array([[1.0/2,1],[-1.0/2,1],[1.0/2,1]])\n",
    "lightcol2 = np.array([[0,1],[1,1],[0,0.5]])\n",
    "#Display rendered results for pear for all materials and light sources and combination of light sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Photometric Stereo, Specularity Removal (20 pts)\n",
    "\n",
    "The goal of this problem is to implement a couple of different algorithms that reconstruct a surface using the concept of Lambertian photometric stereo. Additionally, you will implement the specular removal technique of [Mallick et al.](http://www.eecs.harvard.edu/~zickler/download/photodiff_cvpr05_preprint.pdf), which enables photometric stereo to be performed on certain non-Lambertian materials.\n",
    "\n",
    "You can assume a Lambertian reflectance function once specularities are removed. However, note that the albedo is unknown and non-constant in the images you will use.\n",
    "\n",
    "As input, your program should take in multiple images along with the light source direction for each image. Each image is associated with only a single light, and hence a single direction.\n",
    "\n",
    "### Data\n",
    "You will use synthetic images and specular sphere images as data. These images are stored in `.pickle` files which have been graciously provided by Satya Mallick. Each `.pickle` file contains\n",
    "\n",
    "* `im1`, `im2`, `im3`, `im4`, ... images.\n",
    "* `l1`, `l2`, `l3`, `l4`, ... light source directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Lambertian Photometric Stereo [8 pts]\n",
    "\n",
    "Implement the photometric stereo technique described in the lecture. Your program should have two parts:\n",
    "\n",
    "1. Read in the images and corresponding light source directions, and estimate the surface normals and albedo map.\n",
    "\n",
    "1. Reconstruct the depth map from the surface normals. You should first try the naive scanline-based \"shape by integration\" method described in lecture. (You are required to implement this.) For comparison, you should also integrate using the Horn technique which is already implemented for you in the `horn_integrate` function. Note that for good results you will often want to run the `horn_integrate` function with 10000-100000 iterations, which will take a while. For your final submission, we will require that you run Horn integration for 10000 (ten thousand) iterations or more in each case. But for debugging, it is suggested that you keep the number of iterations low.\n",
    "\n",
    "You will find all the data for this part in `synthetic_data.pickle`. Try using only `im1`, `im2` and `im4` first. Display your outputs as mentioned below.\n",
    "\n",
    "Then use all four images (most accurate).\n",
    "\n",
    "**Note:** **DO NOT** normalize the images prior to use in the photemetric stero algorithm. the images must be used **as-is**.\n",
    "\n",
    "For **each** of the **two above cases** you must output:\n",
    "\n",
    "1. The estimated albedo map.\n",
    "\n",
    "1. The estimated surface normals by showing both\n",
    "    1. Needle map, and\n",
    "    1. Three images showing each of the surface normal components.\n",
    "\n",
    "1. A wireframe of the depth map given by the scanline method.\n",
    "\n",
    "1. A wireframe of the depth map given by Horn integration.\n",
    "\n",
    "In total, we expect 2 * 7 = 14 images for this part.\n",
    "\n",
    "An example of outputs is shown in the figure below. (The example outputs only include one depth map, although we expect two â€“ see above.)\n",
    "\n",
    "<!--- ![Problem 4.1 example outputs](problem4_example.png) --->\n",
    "<!--- The previous results in export to pdf errors on some systems but the following does not --->\n",
    "<img src=\"problem4_example.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Example: how to read and access data from a .pickle file\n",
    "pickle_in = open(\"synthetic_data.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "# data is a dict which stores each element as a key-value pair. \n",
    "print(\"Keys: \", list(data.keys()))\n",
    "\n",
    "# To access the value of an entity, refer to it by its key.\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "for i in range(1, 5):\n",
    "    sub = fig.add_subplot(int('22'+str(i)))\n",
    "    sub.set_title('Image ' + str(i))\n",
    "    sub.set_xlabel(\"Light source direction: \" + str(data[\"l%d\" % i]))\n",
    "    sub.imshow(data[\"im%d\" % i], cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above images, can you interpret the orientation of the coordinate frame? If we label the axes in order as x, y, z, then the x-axis points left, the y-axis points up, and the z-axis points out of the screen in our direction. (That means this is a left-handed coordinate system. How will this affect the scanline integration algorithm? Hint: if you integrate rightward along the x-axis and downward along the y-axis, you will be doing in opposite directions to the axes, and the partial derivatives you compute may need to be modified.)\n",
    "\n",
    "_Note: as clarification, no direct response is needed for this cell._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve\n",
    "\n",
    "def horn_integrate(gx, gy, mask, niter):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - horn_integrate recovers the function g from its partial \n",
    "    derivatives gx and gy. \n",
    "    - mask is a binary image which tells which pixels are \n",
    "    involved in integration. \n",
    "    - niter is the number of iterations, typically between 10,000 - 100,000, \n",
    "    although the trend can be seen even after 1000 iterations.\n",
    "    \"\"\"\n",
    "    g = np.ones(np.shape(gx))\n",
    "    \n",
    "    gx = np.multiply(gx, mask)\n",
    "    gy = np.multiply(gy, mask)\n",
    "    \n",
    "    A = np.array([[0,1,0],[0,0,0],[0,0,0]]) #y-1\n",
    "    B = np.array([[0,0,0],[1,0,0],[0,0,0]]) #x-1\n",
    "    C = np.array([[0,0,0],[0,0,1],[0,0,0]]) #x+1\n",
    "    D = np.array([[0,0,0],[0,0,0],[0,1,0]]) #y+1\n",
    "    \n",
    "    d_mask = A + B + C + D\n",
    "    \n",
    "    den = np.multiply(convolve(mask,d_mask,mode=\"same\"),mask)\n",
    "    den[den == 0] = 1\n",
    "    rden = 1.0 / den\n",
    "    mask2 = np.multiply(rden, mask)\n",
    "    \n",
    "    m_a = convolve(mask, A, mode=\"same\")\n",
    "    m_b = convolve(mask, B, mode=\"same\")\n",
    "    m_c = convolve(mask, C, mode=\"same\")\n",
    "    m_d = convolve(mask, D, mode=\"same\")\n",
    "    \n",
    "    term_right = np.multiply(m_c, gx) + np.multiply(m_d, gy)\n",
    "    t_a = -1.0 * convolve(gx, B, mode=\"same\")\n",
    "    t_b = -1.0 * convolve(gy, A, mode=\"same\")\n",
    "    term_right = term_right + t_a + t_b\n",
    "    term_right = np.multiply(mask2, term_right)\n",
    "    \n",
    "    for k in range(niter):\n",
    "        g = np.multiply(mask2, convolve(g, d_mask, mode=\"same\")) + term_right\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to compute the albego, normals, \n",
    "    and height map using the photometric stereo \n",
    "    method & horn integration'''\n",
    "def photometric_stereo(images, lights, mask, horn_niter=25000):\n",
    "    \n",
    "    \"\"\"\n",
    "    *** Input images should not be normalized to [0, 1] range, use as-is.\n",
    "\n",
    "    *** mask is an optional parameter used to ignore the background when \n",
    "    integrating the normals. In practice something like 0.05 or 0.1 tends to \n",
    "    work well.\n",
    "    \n",
    "    You do not need to use the mask for 1a (it shouldn't matter, just pass a mask of all ones),\n",
    "    but you SHOULD use it to filter out the background for the specular data (1c).\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "\n",
    "    # note:\n",
    "    # images : (n_ims, h, w)\n",
    "    # lights : (n_ims, 3)\n",
    "    # mask   : (h, w)\n",
    "    \n",
    "    albedo = np.ones(images[0].shape)\n",
    "    normals = np.dstack((np.zeros(images[0].shape),\n",
    "                         np.zeros(images[0].shape),\n",
    "                         np.ones(images[0].shape)))\n",
    "    H = np.ones(images[0].shape)\n",
    "    H_horn = np.ones(images[0].shape)\n",
    "    return albedo, normals, H, H_horn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# The following code is just a working example so you don't get stuck with any\n",
    "# of the graphs required. You may want to write your own code to align the\n",
    "# results in a better layout. You are also free to change the function\n",
    "# however you wish; just make sure you get all of the required outputs.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def visualize(albedo, normals, depth, horn, imtitle='', stride = 15):\n",
    "    # showing albedo map\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    albedo_max = albedo.max()\n",
    "    albedo = albedo / albedo_max\n",
    "    ax0 = fig.add_subplot(231)\n",
    "    ax0.set_title(\"Albedo\")\n",
    "    ax0.imshow(albedo, cmap=\"gray\")\n",
    "\n",
    "    # showing normals as three separate channels\n",
    "    ax1 = fig.add_subplot(253)\n",
    "    fig.colorbar(ax1.imshow(normals[..., 0]), ax=ax1, orientation='horizontal')\n",
    "    ax2 = fig.add_subplot(254)\n",
    "    ax2.set_title(\"Normals as 3 separate channels.\")\n",
    "    fig.colorbar(ax2.imshow(normals[..., 1]), ax=ax2, orientation='horizontal')\n",
    "    ax3 = fig.add_subplot(255)\n",
    "    fig.colorbar(ax3.imshow(normals[..., 2]), ax=ax3, orientation='horizontal')\n",
    "\n",
    "    # showing normals as quiver\n",
    "    X, Y, _ = np.meshgrid(np.arange(0,np.shape(normals)[0], stride),\n",
    "                          np.arange(0,np.shape(normals)[1], stride),\n",
    "                          np.arange(1))\n",
    "    X = X[..., 0]\n",
    "    Y = Y[..., 0]\n",
    "    Z = depth[::stride,::stride].T\n",
    "    NX = normals[..., 0][::stride,::-stride].T\n",
    "    NY = normals[..., 1][::-stride,::stride].T\n",
    "    NZ = normals[..., 2][::stride,::stride].T\n",
    "    ax4 = fig.add_subplot(234, projection='3d')\n",
    "    ax4.set_title(\"Needle map\")\n",
    "    ax4.quiver(X,Y,Z,NX,NY,NZ, length=10, color='g')\n",
    "\n",
    "    # plotting wireframe depth map\n",
    "    H = depth[::stride,::stride]\n",
    "    ax5 = fig.add_subplot(235, projection='3d')\n",
    "    ax5.set_title(\"Wireframe - PS\")\n",
    "    ax5.plot_surface(X,Y, H.T, color='g')\n",
    "\n",
    "    H = horn[::stride,::stride]\n",
    "    ax6 = fig.add_subplot(236,projection='3d')\n",
    "    ax6.set_title(\"Wireframe - HORN\")\n",
    "    ax6.plot_surface(X,Y, H.T, color='g')\n",
    "    fig.suptitle(imtitle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to run your photometric stereo code on TWO sets of images!\n",
    "# (One being {im1, im2, im4}, and the other being {im1, im2, im3, im4}.)\n",
    "# Some code is given to you below to help you get started.\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pickle_in = open(\"synthetic_data.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "lights = \n",
    "\n",
    "images = []\n",
    "\n",
    "mask = np.ones(data[\"im1\"].shape) # these images don't have a background, so we'll just use a mask of all ones\n",
    "\n",
    "albedo, normals, depth, horn = photometric_stereo(images, lights, mask)\n",
    "visualize(albedo, normals, depth, horn, \"Photometric Sterio - Synthetic data (3 images)\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Specularity Removal [6 pts]\n",
    "\n",
    "Implement the specularity removal technique described in *Beyond Lambert: Reconstructing Specular Surfaces Using Color* (by Mallick, Zickler, Kriegman, and Belhumeur; CVPR 2005).\n",
    "\n",
    "Your program should input an RGB image and light source color and output the corresponding SUV image.  \n",
    "\n",
    "Try this out first with the specular sphere images and then with the pear images.  \n",
    "  \n",
    "For each of the specular sphere and pear images, include\n",
    "\n",
    "1. The original image (in RGB colorspace).\n",
    "\n",
    "1. The recovered $S$ channel of the image.\n",
    "\n",
    "1. The recovered diffuse part of the image. Use $D = \\sqrt{U^2+V^2}$ to represent the diffuse part.\n",
    "\n",
    "In total, we expect 2 * 3 = 6 images as outputs for this problem.\n",
    "\n",
    "Note: You will find all the data for this part in `specular_sphere.pickle` and `specular_pear.pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot_mat(rot_v, unit=None):\n",
    "    '''\n",
    "    Takes a vector and returns the rotation matrix required to align the\n",
    "    unit vector(2nd arg) to it.\n",
    "    '''\n",
    "    if unit is None:\n",
    "        unit = [1.0, 0.0, 0.0]\n",
    "    \n",
    "    rot_v = rot_v/np.linalg.norm(rot_v)\n",
    "    uvw = np.cross(rot_v, unit) # axis of rotation\n",
    "\n",
    "    rcos = np.dot(rot_v, unit) # cos by dot product\n",
    "    rsin = np.linalg.norm(uvw) # sin by magnitude of cross product\n",
    "\n",
    "    # normalize and unpack axis\n",
    "    if not np.isclose(rsin, 0):\n",
    "        uvw = uvw/rsin\n",
    "    u, v, w = uvw\n",
    "\n",
    "    # compute rotation matrix \n",
    "    R = (\n",
    "        rcos * np.eye(3) +\n",
    "        rsin * np.array([\n",
    "            [ 0, -w,  v],\n",
    "            [ w,  0, -u],\n",
    "            [-v,  u,  0]\n",
    "        ]) +\n",
    "        (1.0 - rcos) * uvw[:,None] * uvw[None,:]\n",
    "    )\n",
    "    return R\n",
    "\n",
    "def RGBToSUV(I_rgb, rot_vec):\n",
    "    '''\n",
    "    Your implementation which takes an RGB image and a vector encoding\n",
    "    the orientation of the S channel w.r.t. to RGB.\n",
    "    '''\n",
    "\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "\n",
    "    S = np.ones(I_rgb.shape[:2])\n",
    "    G = np.ones(I_rgb.shape[:2])\n",
    "    return S, G\n",
    "\n",
    "pickle_in = open(\"specular_sphere.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "# sample input\n",
    "S, G = RGBToSUV(data[\"im1\"], np.hstack((data[\"c\"][0][0],\n",
    "                                        data[\"c\"][1][0],\n",
    "                                        data[\"c\"][2][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Robust Photometric Stereo [6 pts]\n",
    "\n",
    "Now we will perform photometric stereo on our sphere/pear images which include specularities. First, for comparison, run your photometric stereo code from **Part 1** on the original images (converted to grayscale). You should notice erroneous \"bumps\" in the resulting reconstructions, as a result of violating the Lambertian assumption. For this, show the same outputs as in **Part 1**.\n",
    "\n",
    "Next, combine parts 1 and 2 by removing the specularities (using your code from **Part 2**) and then running photometric stereo on the diffuse components of the specular sphere/pear images. Our goal will be to remove the bumps/sharp parts in the reconstruction.\n",
    "\n",
    "**Note:** While creating the masks, please use $0.1$ as your threshold. **DO NOT** \"normalize\" or ortherwise modify the images prior to use in the photometric stereo algorithm. The images must be used **as-is**.\n",
    "\n",
    "For the specular sphere image set in `specular_sphere.pickle` and specular pear images set in `specular_pear.pickle`, using all of four images in each, include:\n",
    "\n",
    "1. The estimated albedo map (original and diffuse).\n",
    "\n",
    "1. The estimated surface normals (original and diffuse) by showing both\n",
    "\n",
    "    1. Needle map, and\n",
    "    1. Three images showing each of the surface normal components.\n",
    "    \n",
    "1. A wireframe of depth map (original and diffuse).\n",
    "\n",
    "1. A wireframe of the depth map given by Horn integration (original and diffuse).\n",
    "\n",
    "In total, we expect 2 \\* 7 = 14 images for the 1a comparison, plus 2 \\* 7 = 14 images for the outputs after specularity removal has been performed. (Thus 28 output images overall.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# You may reuse the code for photometric_stereo here.\n",
    "# Write your code below to process the data and send it to photometric_stereo\n",
    "# and display the albedo, normals, and depth maps.\n",
    "# ---------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
